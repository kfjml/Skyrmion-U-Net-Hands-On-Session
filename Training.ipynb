{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a02ba23-bf45-4190-b33e-bb6ea78eb684",
   "metadata": {},
   "source": [
    "# Skyrmion U-Net training example (mini U-Net)\n",
    "\n",
    "The first part will involve training a mini U-Net model with a small dataset, making it feasible to train on a CPU within a realistic time.\n",
    "## Import packages + define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224fabf-49ec-42b4-b905-c3cdf77d2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import albumentations\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "def plotfig(l,ltitle=None,nrows=None,ncols=None,dpi=100,s0=1,suptitle=None):\n",
    "    if nrows==None and ncols==None:\n",
    "        nc = int(np.ceil(np.sqrt(len(l))))\n",
    "        nr = int(np.ceil(len(l)/nc))\n",
    "    if ncols!=None and ncols!=None:\n",
    "        nc = ncols\n",
    "        nr = nrows\n",
    "    if nrows!=None and ncols==None:\n",
    "        nr = nrows\n",
    "        nc = int(np.ceil(len(l)/nr))\n",
    "    if ncols!=None and nrows==None:\n",
    "        nc = ncols\n",
    "        nr = int(np.ceil(len(l)/nc))\n",
    "    fig,ax = plt.subplots(nrows=nr,ncols=nc,dpi=dpi,figsize=(nc*s0,nr*s0))\n",
    "    if suptitle != None:\n",
    "        fig.suptitle(suptitle,fontsize=40,y=0.99)\n",
    "    ax = ax.ravel()\n",
    "    for i in range(len(ax)):\n",
    "        ax[i].axis(\"off\")\n",
    "    \n",
    "    for i in range(min(len(l),len(ax))):\n",
    "        ax[i].imshow(l[i],cmap=\"gray\")\n",
    "        if ltitle!= None:\n",
    "            ax[i].set_title(ltitle[i])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34393e4-757a-4ff2-a1b9-36b4aafbb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns true if a GPU is available\n",
    "gpu_available = lambda : len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "#in the case of GPU, switch to mixed_float16 policy\n",
    "if gpu_available():\n",
    "    print(\"GPU available\")\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6965f2-88e4-4de4-b91d-444964dcb100",
   "metadata": {},
   "source": [
    "## \"mini\" U-Net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dab505-2c0a-4524-8fb5-c844dbabe854",
   "metadata": {},
   "source": [
    "![](notebook_figures/u_net_architecture_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e5d13-f818-4d13-ae52-264840d1db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic activation layer\n",
    "class MishLayer(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.keras.activations.mish(x)\n",
    "        \n",
    "# Basic Convolution Block\n",
    "def conv_block(x, n_channels, param):\n",
    "    x = tf.keras.layers.Conv2D(n_channels, kernel_size=param[\"kernel_size\"],kernel_initializer=param[\"kernel_initialization\"],padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x) \n",
    "    x = MishLayer()(x)\n",
    "    return x\n",
    "\n",
    "# Double Convolution Block used in \"encoder\" and \"bottleneck\"\n",
    "def double_conv_block(x, n_channels, param):\n",
    "    x = conv_block(x,n_channels,param)\n",
    "    x = conv_block(x,n_channels,param)\n",
    "    return x\n",
    "\n",
    "# Downsample block for feature extraction (encoder)\n",
    "def downsample_block(x, n_channels, param):\n",
    "    f = double_conv_block(x, n_channels, param)\n",
    "    p = tf.keras.layers.MaxPool2D(pool_size=(2,2))(f)\n",
    "    p = tf.keras.layers.Dropout(param[\"dropout\"])(p)\n",
    "    return f, p\n",
    "\n",
    "# Upsample block for the decoder\n",
    "def upsample_block(x, conv_features, n_channels, param):\n",
    "    x = tf.keras.layers.Conv2DTranspose(n_channels*param[\"upsample_channel_multiplier\"], param[\"kernel_size\"], strides=(2,2), padding='same')(x)\n",
    "    x = tf.keras.layers.concatenate([x, conv_features])\n",
    "    x = tf.keras.layers.Dropout(param[\"dropout\"])(x)\n",
    "    x = double_conv_block(x, n_channels, param)\n",
    "    return x\n",
    "\n",
    "# Create the model\n",
    "def get_unet(param):\n",
    "    input = tf.keras.layers.Input(shape=param[\"input_shape\"]+(1,))\n",
    "    next_input = input\n",
    "    \n",
    "    l_residual_con = []\n",
    "    for i in range(param[\"n_depth\"]):\n",
    "        residual_con,next_input = downsample_block(next_input, (2**i)*param[\"filter_multiplier\"],param)\n",
    "        l_residual_con.append(residual_con)\n",
    "\n",
    "    next_input = double_conv_block(next_input, (2**param[\"n_depth\"])*param[\"filter_multiplier\"],param)\n",
    "\n",
    "    for i in range(param[\"n_depth\"]):\n",
    "        next_input = upsample_block(next_input, l_residual_con[param[\"n_depth\"]-1-i], (2**(param[\"n_depth\"]-1-i))*param[\"filter_multiplier\"],param)\n",
    "\n",
    "    output = tf.keras.layers.Conv2D(param[\"n_class\"], (1,1), padding=\"same\", activation = \"softmax\",dtype='float32')(next_input)    \n",
    "    \n",
    "    return tf.keras.Model(input, output, name=param[\"name\"])\n",
    "\n",
    "model = get_unet({\"name\":\"unet\",\"input_shape\": (512,672), \"n_class\":3,\"filter_multiplier\":5,\"n_depth\":1,\n",
    "                  \"kernel_initialization\":\"he_normal\",\"dropout\":0.01,\"kernel_size\":(8,8),\"upsample_channel_multiplier\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62c121-2a30-4411-87fa-54665225059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf6041-72c6-4277-9a45-5f8cadb46363",
   "metadata": {},
   "source": [
    "## Dataset processing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35979fb-49d4-4fa7-9b23-95b88e7c7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_table = pd.read_csv(\"dataset/table.csv\",sep=\";\")\n",
    "dataset_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43016e-d7b2-4a85-8d7c-2abb1c3bc142",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958381d-8c5c-4203-bb75-264730c0fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Training'\n",
    "    def __init__(self, images, labels, batch_size, n_class=3, smoothing=False,shuffle=True, aug=None):\n",
    "        super().__init__()\n",
    "        'Initialization'\n",
    "        self.n_class = n_class\n",
    "        if labels.shape[:3] != images.shape[:3]:\n",
    "            raise Exception(\"Shape not fit\")\n",
    "\n",
    "        self.len_data = labels.shape[0]\n",
    "        self.shape_data = labels.shape[1:3]\n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "        self.aug = aug\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len_data)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.len_data/ self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X,y = (self.images[indexes]).copy(), (self.labels[indexes]).copy()    \n",
    "        if self.aug is not None:      \n",
    "            self.data_augmentation(X, y)\n",
    "        y = y.astype(dtype=np.float16)\n",
    "        if self.smoothing:\n",
    "            y[y==0] = self.smooth_labels(0.0,factor=0.2)\n",
    "            y[y==1] = self.smooth_labels(1.0,factor=0.2)\n",
    "        return X/255,y\n",
    "        \n",
    "    def smooth_labels(self, labels, factor=0.1):\n",
    "        return labels*(1 - factor)+(factor / self.n_class)\n",
    "    \n",
    "    def data_augmentation(self, X, y):\n",
    "        for i in range(self.batch_size):\n",
    "            augmented = self.aug(image=X[i],mask=y[i])\n",
    "            X[i] = augmented[\"image\"]\n",
    "            y[i] = augmented[\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f91d3-5277-4997-80a1-577bd1743d9e",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c7f8c-ac7d-4f93-b744-219fb857697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.GaussNoise(p=1,var_limit=20**2),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.5, scale_limit=(-0.15,0.4), rotate_limit=90, p=1),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.25,contrast_limit=0.25,p=1)], p=1)\n",
    "\n",
    "aug = get_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753aced-d5f6-46a7-ad75-b84c109b87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexample_img = []\n",
    "lexample_aug = []\n",
    "for ix,subtable in dataset_table.groupby(\"source_id\"):\n",
    "    if ix not in [7, 8, 10, 11, 13, 23, 135]: continue\n",
    "    img = (np.array(Image.open(subtable.iloc[0].img_fn)))  \n",
    "    lexample_img.append(img)\n",
    "    lexample_aug.append(aug(image=img)[\"image\"])\n",
    "plotfig(lexample_img+lexample_aug,nrows=2,dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331f988-b99d-471a-97eb-8671ab6bdb5b",
   "metadata": {},
   "source": [
    "### Splitting of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135988d-1143-4965-8670-50cdef0f898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ix = np.array([ix for ele in sorted(list(set([10,11,20,23,7,135]))) for ix in dataset_table[dataset_table.source_id==ele].index.to_numpy()])\n",
    "test_ix = np.array([ix for ele in sorted(list(set([6,9]))) for ix in dataset_table[dataset_table.source_id==ele].index.to_numpy()])\n",
    "val_ix = np.array([ix for ele in sorted(list(set([8,13]))) for ix in dataset_table[dataset_table.source_id==ele].index.to_numpy()])\n",
    "\n",
    "def group_after_source(lix,ncolmax=6,suptitle=None):\n",
    "    table = dataset_table.iloc[lix]\n",
    "    plotfig([np.array(Image.open(b.iloc[0].img_fn)) for a,b in table.groupby(\"source_id\")],[str(a) for a,b in table.groupby(\"source_id\")],suptitle=suptitle,ncols=6,dpi=100,s0=5)\n",
    "group_after_source(train_ix,suptitle=\"Training set\")\n",
    "group_after_source(test_ix,suptitle=\"Test set\")\n",
    "group_after_source(val_ix,suptitle=\"Validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fd525-5f65-4cbc-9968-31fd86959930",
   "metadata": {},
   "source": [
    "### Loading images and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a35cf-38de-4e03-b72d-75c9c731d079",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "\n",
    "Labels for U-Net model:\n",
    "- Skyrmions (RGB-label: red (255,0,0))\n",
    "- Defects (RGB-label: green (0,255,0)\n",
    "- FM background (RGB-label: blue (0,0,255))\n",
    "- Non-FM background (RGB-label: yellow (255,255,0))\n",
    "- Boundary non-FM/FM background (RGB-label: cyan (0,255,255))\n",
    "\n",
    "The 3-class U-Net model predicts skyrmions (RGB-label: red (255,0,0)), defects (RGB-label: blue (0,255,0)), and background (RGB-label: blue (0,0,255)). The background consists of the ferromagnetic background, non-ferromagnetic background, and the boundary between ferromagnetic and non-ferromagnetic background. The RGB-label is converted to an class index for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ddd17-4c97-405f-807b-8f723c54e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trafo_rgb_to_channel(I):\n",
    "    Q = np.zeros((I.shape[0],I.shape[1]),dtype=np.uint8)\n",
    "    R,G,B = I[:,:,0],I[:,:,1],I[:,:,2]\n",
    "    skyrmion_mask = (R>=128)&(G<128)&(B<128)\n",
    "    defect_mask = (R<128)&(G>=128)&(B<128)\n",
    "    bck_mask = ~(skyrmion_mask|defect_mask)\n",
    "    Q[skyrmion_mask] = 0\n",
    "    Q[defect_mask] = 1\n",
    "    Q[bck_mask] = 2\n",
    "    return Q\n",
    "    \n",
    "def load_img_label_data(lix):\n",
    "    return np.array([Image.open(ele) for ele in dataset_table.iloc[lix].img_fn]), \\\n",
    "           np.array([trafo_rgb_to_channel(np.array(Image.open(ele))) for ele in dataset_table.iloc[lix].label_fn])\n",
    "\n",
    "train_img,train_label = load_img_label_data(train_ix)\n",
    "val_img,val_label = load_img_label_data(val_ix)\n",
    "test_img,test_label = load_img_label_data(test_ix)\n",
    "img_size = test_img.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd754071-aac4-4b15-b214-b8cf22452aab",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Loss & Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4462d8-2119-4c36-ae4d-7c5118d36fc6",
   "metadata": {},
   "source": [
    "U-Net can be described as follows:\n",
    "\n",
    "$$ \\mathbf{z} = \\mathbf{f}_{\\vec{\\theta}}(\\mathbf{x}) $$\n",
    "\n",
    "where $\\vec{\\theta}$ is a high-dimensional vector containing all parameters, $\\mathbf{z}$ is a 3-dimensional output tensor, and $\\mathbf{x}$ is the input Kerr microscopy image. In $\\mathbf{f}$ are encapsulated all the operations such as convolution, max pooling, up-convolution, batch normalization, ... .\n",
    "\n",
    "$\\mathbf{z}$ is converted to probabilities using softmax:\n",
    "\n",
    "$$ p_{(x,y),i} = \\frac{\\exp(\\mathbf{z}_{(x,y),i})}{\\sum_{i\\in \\mathrm{classes}} \\exp(\\mathbf{z}_{(x,y),i})} $$\n",
    "\n",
    "The output mask $\\mathbf{m}$ is then determined by:\n",
    "\n",
    "$$ \\mathbf{m}_{(x,y)} = \\mathrm{arg\\,max}_i\\; p_{(x,y),i}$$\n",
    "\n",
    "The cross-entropy loss (with averaging over pixels) is given by:\n",
    "\n",
    "$$ L = - \\frac{1}{\\sum_{x,y} 1} \\sum_{x,y} \\sum_{i=1}^3 \\;w_i\\;(p_{\\mathrm{ground-truth}})_{(x,y),i} \\log(p_{(x,y),i}) $$\n",
    "\n",
    "where $\\sum_{(x,y)}$ in our case sums over several examples.\n",
    "\n",
    "The training of a neural network aims to obtain the global minimum, i.e., it satisfies $\\nabla_{\\vec{\\theta}} L = 0$. To achieve this, the network needs to be trained. The simplest approach is:\n",
    "\n",
    "$$ \\vec{\\theta}_{i+1} = \\vec{\\theta}_{i} - \\epsilon \\left. \\nabla_{\\vec{\\theta}} L \\right|_{\\vec{\\theta}_i} $$\n",
    "\n",
    "However, in this session, we utilized more advanced and superior training algorithms.\n",
    "\n",
    "The gradient $\\nabla_{\\vec{\\theta}} L$ can be calculated using backpropagation, which relies on the chain rule for derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100e450-ba6d-464f-893b-275f05eed66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(weight):\n",
    "    weight = tf.convert_to_tensor(weight,dtype=np.float32)\n",
    "    def loss(ytrue,ypred):\n",
    "        p = tf.clip_by_value(ypred/(tf.math.reduce_sum(ypred,axis=-1,keepdims=True)),tf.keras.backend.epsilon(),1-tf.keras.backend.epsilon())\n",
    "        return -tf.math.reduce_mean(tf.math.reduce_sum(weight*ytrue*tf.math.log(p),axis=-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e38b50-04a7-4072-bc13-a2a2683aeeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TF_PN(y_true,y_pred,ix0):\n",
    "    m1,m2 = y_true==ix0,y_pred==ix0\n",
    "    im1,im2 = tf.math.logical_not(m1),tf.math.logical_not(m2)\n",
    "    TP = tf.math.reduce_mean(tf.cast(tf.math.logical_and(m1,m2),dtype=np.float64))\n",
    "    TN = tf.math.reduce_mean(tf.cast(tf.math.logical_and(im1,im2),dtype=np.float64))\n",
    "    FP = tf.math.reduce_mean(tf.cast(tf.math.logical_and(im1,m2),dtype=np.float64))\n",
    "    FN = tf.math.reduce_mean(tf.cast(tf.math.logical_and(m1,im2),dtype=np.float64))\n",
    "    return TP,TN,FP,FN\n",
    "\n",
    "def get_mcc_from_TF_PN(TP,TN,FP,FN):\n",
    "    denom = tf.keras.ops.sqrt((TP + FN) * (FP + TN) * (FP + TP) * (FN + TN))\n",
    "    val = (TP * TN - FP * FN) / denom\n",
    "    return  tf.where(tf.equal(denom, 0), tf.constant(0, dtype=tf.float64), val)\n",
    "\n",
    "get_mcc = lambda x,y,ix0: get_mcc_from_TF_PN(*get_TF_PN(x,y,ix0)).numpy()\n",
    "\n",
    "#Matthews correlation coefficient\n",
    "def get_mcc_cpu(a,b,ix0):\n",
    "    #a=true label,b=predicted label\n",
    "    m1,m2 = (a==ix0).flatten(),(b==ix0).flatten()\n",
    "    im1,im2 = np.invert(m1),np.invert(m2)\n",
    "    TP = np.sum(np.logical_and(m1,m2))/(len(m1))\n",
    "    TN = np.sum(np.logical_and(im1,im2))/(len(m1))\n",
    "    FP = np.sum(np.logical_and(im1,m2))/(len(m1))\n",
    "    FN = np.sum(np.logical_and(m1,im2))/(len(m1))\n",
    "    if (TP+FN)*(FP+TN)*(FP+TP)*(FN+TN)==0:\n",
    "        return np.nan\n",
    "    return (TP*TN-FP*FN)/(np.sqrt((TP+FN)*(FP+TN)*(FP+TP)*(FN+TN)))\n",
    "\n",
    "class MCC(tf.keras.metrics.Metric):\n",
    "    def __init__(self, ix0, name=\"MCC\", **kwargs):\n",
    "        super().__init__(name=name,dtype=tf.float64,**kwargs)\n",
    "        self.ix0 = ix0\n",
    "        self.tp = self.add_variable(shape=(),name=\"TP\", initializer=\"zeros\",dtype=tf.float64)\n",
    "        self.tn = self.add_variable(shape=(),name=\"TN\", initializer=\"zeros\",dtype=tf.float64)\n",
    "        self.fp = self.add_variable(shape=(),name=\"FP\", initializer=\"zeros\",dtype=tf.float64)\n",
    "        self.fn = self.add_variable(shape=(),name=\"FN\", initializer=\"zeros\",dtype=tf.float64)\n",
    "        self.tot = self.add_variable(shape=(),name=\"TOT\", initializer=\"zeros\",dtype=tf.int64)\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        TP,TN,FP,FN = get_TF_PN(tf.argmax(y_true,axis=-1),tf.argmax(y_pred,axis=-1),self.ix0)\n",
    "        totn1 = tf.cast(self.tot+1,tf.float64)\n",
    "        self.tp.assign_add((TP-self.tp)/totn1)\n",
    "        self.tn.assign_add((TN-self.tn)/totn1)\n",
    "        self.fp.assign_add((FP-self.fp)/totn1)\n",
    "        self.fn.assign_add((FN-self.fn)/totn1)\n",
    "        self.tot.assign_add(1)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.tp.assign(0)\n",
    "        self.tn.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        self.fn.assign(0)\n",
    "        self.tot.assign(0)    \n",
    "    \n",
    "    def result(self):\n",
    "        return get_mcc_from_TF_PN(self.tp,self.tn,self.fp,self.fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f11ab0-c6c0-4cf6-ba2f-48fa0866182c",
   "metadata": {},
   "source": [
    "### Initialization and performing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e502439-1e23-4aa9-bb59-2d5d875cd7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeln = \"model_1\"\n",
    "folder = \"./training/\"\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "modelfn = folder+modeln+\".keras\"\n",
    "\n",
    "model = get_unet({\"name\":\"unet\",\"input_shape\": (512,672), \"n_class\":3,\"filter_multiplier\":5,\"n_depth\":1,\n",
    "                  \"kernel_initialization\":\"he_normal\",\"dropout\":0.01,\"kernel_size\":(8,8),\"upsample_channel_multiplier\":2})\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),loss=get_cross_entropy_loss([6,1,1]),metrics=[\"accuracy\",MCC(0)])\n",
    "\n",
    "batch_size = 5\n",
    "basis = np.eye(3,dtype=np.uint8)\n",
    "dg_train = DataGenerator(train_img,basis[train_label],batch_size,smoothing=False,aug=get_augmentation())\n",
    "dg_val = DataGenerator(val_img,basis[val_label],batch_size,smoothing=False,aug=get_augmentation())\n",
    "\n",
    "history = model.fit(x=dg_train,validation_data=dg_val,epochs=15,verbose=1,batch_size=batch_size,callbacks=[\n",
    "tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min',factor=0.5, patience=2, min_lr=1e-6, verbose=1),\n",
    "tf.keras.callbacks.ModelCheckpoint(modelfn, verbose=1,monitor='val_loss',mode='min',save_best_only=True,save_weights_only=False)])\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)  \n",
    "hist_df.to_csv(folder+modeln+\"_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb78668e-e32b-497f-8e02-dace9dfced45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,figsize=(10,3.5),dpi=300)\n",
    "fig.suptitle(\"Training history - Validation Set\",fontsize=14)\n",
    "ax[0].plot(hist_df[\"val_loss\"])\n",
    "ax[1].plot(hist_df[\"val_accuracy\"])\n",
    "ax[2].plot(hist_df[\"val_MCC\"])\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Cross-entropy loss\")\n",
    "ax[0].set_title(\"Cross-entropy loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[2].set_ylabel(\"MCC\")\n",
    "ax[2].set_title(\"MCC\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3,figsize=(10,3.5),dpi=300)\n",
    "fig.suptitle(\"Training history - Trainings Set\",fontsize=14)\n",
    "ax[0].plot(hist_df[\"loss\"])\n",
    "ax[1].plot(hist_df[\"accuracy\"])\n",
    "ax[2].plot(hist_df[\"MCC\"])\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Cross-entropy loss\")\n",
    "ax[0].set_title(\"Cross-entropy loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[2].set_ylabel(\"MCC\")\n",
    "ax[2].set_title(\"MCC\")\n",
    "fig.tight_layout()\n",
    "\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4865d5f-eb24-4e55-a830-9467567145da",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d703e-baeb-454b-8bbb-6f1a84758b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the label based on Kerr images and the U-Net model.\n",
    "def predict(x,modelfn,batch_size=5,normalize_255=True):\n",
    "    model = tf.keras.models.load_model(modelfn,compile=False,custom_objects={'MishLayer': MishLayer})\n",
    "    n = int(np.ceil(len(x)/batch_size))\n",
    "    lix = [np.array(range(j*batch_size,min((j+1)*batch_size,len(x)))) for j in range(n)]\n",
    "    ylabel = np.zeros(x.shape,dtype=np.uint8)\n",
    "    progbar = tf.keras.utils.Progbar(n)\n",
    "    for i in range(n):            \n",
    "        progbar.update(i)\n",
    "        input = x[lix[i]]\n",
    "        if normalize_255:\n",
    "            input = input/255\n",
    "        ylabel[lix[i]] = model.predict(input,verbose=False).argmax(-1)\n",
    "    progbar.update(n,finalize=True)\n",
    "    return ylabel\n",
    "\n",
    "def trafo_channel_to_rgb(I):\n",
    "    basis = np.array([[255,0,0],[0,255,0],[0,0,255],[255,255,0],[0,255,255]],dtype=np.uint8)\n",
    "    return basis[I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44206e-3fc0-4249-884c-c75849f874bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pixelwise Matthews correlation coefficient on test set (true=skyrmion,false=defect,background)\",get_mcc(val_label,predict(val_img,modelfn,5),0))\n",
    "total_ix = np.hstack((test_ix,val_ix,train_ix))\n",
    "total_img = np.vstack((test_img,val_img,train_img))\n",
    "total_label = np.vstack((test_label,val_label,train_label))\n",
    "total_pred = predict(total_img,modelfn,5)\n",
    "print(\"Pixelwise Matthews correlation coefficient on complete set (true=skyrmion,false=defect,background)\",get_mcc(total_label,total_pred,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99c081-fb1c-458d-8c9f-260e99153b66",
   "metadata": {},
   "source": [
    "### Prediction examples\n",
    "Calculate a prediction for a frame from each video of which frames occur in the dataset.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1939805-65a1-4bd2-9abd-60fb87843656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S = set(np.hstack((test_ix,val_ix,train_ix)))&set([b.iloc[0].name for a,b in dataset_table.groupby(\"source_id\")])\n",
    "for ix0 in range(len(total_img)):\n",
    "    if total_ix[ix0] not in S: continue\n",
    "        \n",
    "    fig,ax = plt.subplots(ncols=3,figsize=(10,3),dpi=200)\n",
    "    for i in range(3):\n",
    "        ax[i].axis(\"off\")\n",
    "    ax[0].imshow(total_img[ix0],cmap=\"gray\")\n",
    "    ax[1].imshow(trafo_channel_to_rgb(total_label[ix0]))\n",
    "    ax[2].imshow(trafo_channel_to_rgb(total_pred[ix0]))\n",
    "    ax[0].set_title(\"Kerr image\")\n",
    "    ax[1].set_title(\"Ground truth\")\n",
    "    ax[2].set_title(\"Predicted label\")\n",
    "    if total_ix[ix0] in train_ix:\n",
    "        fig.suptitle(\"Image used in training\",y=1.02,fontsize=16)\n",
    "    elif total_ix[ix0] in test_ix:\n",
    "        fig.suptitle(\"Image used in testing\",y=1.02,fontsize=16)\n",
    "    elif total_ix[ix0] in val_ix:\n",
    "        fig.suptitle(\"Image used in validation\",y=1.02,fontsize=16)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be36ba-17ee-4a24-ac49-4e8c866afe5b",
   "metadata": {},
   "source": [
    "# Skyrmion U-Net training example (large U-Net)\n",
    "\n",
    "This part shows how to train a large U-Net model, with a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97241f7-221a-4719-a4ad-bc325f4806de",
   "metadata": {},
   "source": [
    "## Download of Zenodo skyrmion U-Net repository\n",
    "Comment this out only if you are going to train the large U-Net. For this purpose, approximately 1GB of data will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7a06b-eb41-4da9-9844-044a46bcb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import wget\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "zenodo_folder = \"./zenodo_dataset/\"\n",
    "if not os.path.exists(zenodo_folder):\n",
    "    os.makedirs(zenodo_folder)\n",
    "\n",
    "wget.download(\"https://zenodo.org/records/10997175/files/public_unet_skyrmion_dataset.zip\",zenodo_folder+\"zenodo_unet_skyrmion_dataset.zip\")\n",
    "with zipfile.ZipFile(zenodo_folder+\"zenodo_unet_skyrmion_dataset.zip\",\"r\") as zip:\n",
    "    zip.extractall(zenodo_folder)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e645c-2d78-43ec-bcee-ac6da4e130fd",
   "metadata": {},
   "source": [
    "## Load dataset = images & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a715b3d-8c26-4619-a960-eb7a3d204d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenodo_dataset_folder = \"./zenodo_dataset/public_unet_skyrmion_dataset/\"\n",
    "zenodo_dataset = pd.read_csv(zenodo_dataset_folder+\"table.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb1e3d-7170-4b77-99a6-405a7031d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfnimg = sorted(list(glob.iglob(zenodo_dataset_folder+\"images/*.png\")))\n",
    "lfnlabel = sorted(list(glob.iglob(zenodo_dataset_folder+\"labels/*.png\")))\n",
    "limg = np.array([np.array(Image.open(ele)) for ele in lfnimg])\n",
    "llabel = np.array([trafo_rgb_to_channel(np.array(Image.open(ele))) for ele in lfnlabel])\n",
    "img_size = limg.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9a1a4-1b05-4e4f-8d81-77dad64e6e51",
   "metadata": {},
   "source": [
    "## Splitting of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da269a-536f-437f-9c41-86deb75de598",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(zenodo_dataset_folder+\"partition.txt\",\"r\") as f:\n",
    "    S = f.read()\n",
    "traintest_batch = []\n",
    "trainonly_batch = []\n",
    "for ele in S.split(\"\\n\")[:-1]:\n",
    "    lt = ele.split(\";\")\n",
    "    if lt[0]==\"train_test_val\":\n",
    "        traintest_batch.append([int(ele) for ele in lt[1:]])\n",
    "    elif lt[0]==\"only_training\":\n",
    "        trainonly_batch.append([int(ele) for ele in lt[1:]])\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "\n",
    "#Association of batches to training, validation and testing\n",
    "train_batch,val_batch,test_batch = [0,1,4,5],[2],[3]\n",
    "#Assignment of indices for each training object (image and label) to validation, test, and training\n",
    "train_sources = [ele1 for ele in trainonly_batch for ele1 in ele]+[ele for ix in train_batch for ele in traintest_batch[ix]]\n",
    "train_ix = np.array(sorted([ele1 for ele in train_sources for ele1 in zenodo_dataset[zenodo_dataset[\"source_id\"]==ele].index]))\n",
    "val_sources = [ele for ix in val_batch for ele in traintest_batch[ix]]\n",
    "val_ix = np.array(sorted([ele1 for ele in val_sources for ele1 in zenodo_dataset[zenodo_dataset[\"source_id\"]==ele].index]))\n",
    "test_sources = [ele for ix in test_batch for ele in traintest_batch[ix]]\n",
    "test_ix = np.array(sorted([ele1 for ele in test_sources for ele1 in zenodo_dataset[zenodo_dataset[\"source_id\"]==ele].index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1c09a-b8d5-4e31-9fa7-baa7b02e817d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Initialisation of Augmentation & Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f293668-afb3-42e5-ae4c-fec4c0f48797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size refers to how many images will be passed through the U-Net at the same time during training\n",
    "#One must adjust the batch_size based on available RAM/VRAM.\n",
    "batch_size = 5\n",
    "    \n",
    "#stronger augmentation, compared to the mini U-Net\n",
    "def get_augmentation():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.GaussNoise(p=1,var_limit=20**2),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.5, scale_limit=(-0.6,0.6), rotate_limit=90, p=1),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=0.3,contrast_limit=0.5,p=1)], p=1)\n",
    "\n",
    "basis = np.eye(3,dtype=np.uint8)\n",
    "#creation of data generators for train and test\n",
    "dg_train = DataGenerator(limg[train_ix],basis[llabel[train_ix]],batch_size,smoothing=False,aug=get_augmentation())\n",
    "dg_val = DataGenerator(limg[val_ix],basis[llabel[val_ix]],batch_size,smoothing=False,aug=get_augmentation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b746b5c2-4842-4d8b-99de-2aa5476774b3",
   "metadata": {},
   "source": [
    "## U-Net architecture & generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f5a82-e970-4ebc-8490-241e0372ece3",
   "metadata": {},
   "source": [
    "![](notebook_figures/u_net_architecture_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3cf99-34c5-4a1f-8751-d27d8d8b5f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modeln = \"model_2\"\n",
    "folder = \"./training/\"\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "modelfn = folder+modeln+\".keras\"\n",
    "\n",
    "model = get_unet({\"name\":\"unet\",\"input_shape\": img_size, \"n_class\":3,\"filter_multiplier\":16,\"n_depth\":4,\n",
    "                  \"kernel_initialization\":\"he_normal\",\"dropout\":0.1,\"kernel_size\":(3,3),\"upsample_channel_multiplier\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e764c18-d377-4df6-932e-bf3fba41494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693040c-cd3a-41d3-8598-bd0c53ade9da",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7deb96-6bbd-42cb-9b07-54b20d228784",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=get_cross_entropy_loss([6.2,10,1.2]),metrics=[\"accuracy\",MCC(0)])\n",
    "history = model.fit(x=dg_train,validation_data=dg_val,epochs=15,verbose=1,batch_size=batch_size,callbacks=[\n",
    "tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min',factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "tf.keras.callbacks.ModelCheckpoint(modelfn, verbose=1,monitor='val_loss',mode='min',save_best_only=True,save_weights_only=False)])\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)  \n",
    "hist_df.to_csv(folder+modeln+\"_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a0d7f0-008f-4b5d-98d7-79950b6de689",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3eaf6a-09f9-448e-ac4d-dc2c40aac92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_img = limg[test_ix]\n",
    "t_pred = predict(t_img,modelfn,5)\n",
    "t_true = llabel[test_ix]\n",
    "#alternative: get_mcc on gpu, if gpu VRAM is large enough\n",
    "print(\"Pixelwise Matthews correlation coefficient on test set (true=skyrmion,false=defect,background)\",get_mcc_cpu(t_true,t_pred,0))\n",
    "lpred = predict(limg,modelfn,5)\n",
    "print(\"Pixelwise Matthews correlation coefficient on complete set (true=skyrmion,false=defect,background)\",get_mcc_cpu(llabel,lpred,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892ad23-f839-4be8-b6f2-53cd47a561ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example plot of a prediction from the test set\n",
    "fig,ax = plt.subplots(ncols=3,dpi=300)\n",
    "ix0 = 2890\n",
    "ax[0].imshow(limg[ix0],cmap=\"gray\")\n",
    "ax[1].imshow(trafo_channel_to_rgb(llabel[ix0]))\n",
    "ax[2].imshow(trafo_channel_to_rgb(lpred[ix0]))\n",
    "ax[0].set_title(\"Kerr image\")\n",
    "ax[1].set_title(\"Ground truth\")\n",
    "ax[2].set_title(\"Predicted label\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b71475-e2da-4893-9e41-21ebe2f6b64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
